{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fea9eb-60b6-4345-bd56-d588a3b8baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Text mining 텍스트마이닝\n",
    "\n",
    "# 1. 키워드 빈도수 분석\n",
    "# 2. 키워드 연관 분석\n",
    "# 3. 인터넷 웹페이지에 있는 이미지, 텍스트, 정보들을 스크랩해올꺼야 => 웹 크롤링\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fef124-dc54-4976-b6ae-23e33b2a377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "웹 크롤링\n",
    "\n",
    "1. 개발자 모드로 변경 : F12 - 도구더보기 - 개발자도구\n",
    "\n",
    "2. 3개의 라이브러리 설치\n",
    "\n",
    "!pip install requests beautifulsoup4 lxml\n",
    "\n",
    "!pip install cloudscraper                              -- 오류발생해서 이걸로 우회해야됨 기존에는 requests 로 불러왔었음\n",
    "\n",
    "3. \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ee5ef-83d7-4415-bd7c-e2ea32b6e56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e0bb0e-396c-4d0f-a03b-182274eaa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\5-17\\jupylab\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\5-17\\jupylab\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\5-17\\jupylab\\lib\\site-packages (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb94106-d63b-449f-a069-b3ed06487dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudscraper\n",
      "  Downloading cloudscraper-1.2.58-py2.py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from cloudscraper) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from cloudscraper) (2.4.7)\n",
      "Collecting requests-toolbelt>=0.9.1\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\5-17\\jupylab\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2021.5.30)\n",
      "Installing collected packages: requests-toolbelt, cloudscraper\n",
      "Successfully installed cloudscraper-1.2.58 requests-toolbelt-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b610c9e-01d4-49f1-a3a6-c24783dd0aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6d76e1-eee4-418c-8b47-bbb3edb98f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6644f-3c69-4659-9ef4-707aa9806d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 신규 코드 ###\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()\n",
    "req = scraper.get(source_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20720c88-336b-480c-b379-02ca423ca443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup                    # bs4 를 이용해서 HTML 의 정보들을 파싱(문법적 분석)\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "scraper = cloudscraper.create_scraper()\n",
    "req = scraper.get(source_url)\n",
    "html = req.content                                                #  html 에 전부 가져옴 구조 상관없이\n",
    "soup = BeautifulSoup(html, 'lxml')                            #   막가져온 걸  뷰티풀 솝 방법으로 파싱(문법적 분석)\n",
    "contents_table = soup.find(name=\"table\")\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ac5bf7-f3bd-4851-b5af-4a949d9d1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://namu.wiki/w/%EA%B4%91%EC%A3%BC%20%ED%95%99%EB%8F%99%20%EA%B1%B4%EB%AC%BC%20%EB%B6%95%EA%B4%B4%20%EC%82%AC%EA%B3%A0\n",
      "https://namu.wiki/w/%EA%B0%95%EB%B6%81%EA%B3%A0%EB%93%B1%ED%95%99%EA%B5%90\n",
      "https://namu.wiki/w/%EC%88%98%EB%82%98%EB%9D%BC\n",
      "https://namu.wiki/w/%EC%82%AC%EB%A7%88%EA%B7%80(%EC%A7%88%EB%B3%91)\n",
      "https://namu.wiki/w/%EC%A0%84%ED%98%84%EB%AC%B4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all('td')[0]\n",
    "    td_url = first_td.find_all('a')\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get('href')\n",
    "        if 'png' not in page_url:\n",
    "            page_urls.append(page_url)\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))\n",
    "for page in page_urls[:5]:\n",
    "    print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad861d5f-849f-4f60-80c1-d368dbd57668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69269b67-3e09-4ce0-9cd7-943ce7733968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "광주 학동 건물 붕괴 사고 \n",
      "\n",
      "\n",
      "붕괴 사고대한민국의 재난2021년 재난광주광역시의 사건사고\n",
      "\n",
      "\n",
      "CCTV 화면 갱신, 문서명 변경에 대한이 문서에서 토론이 진행되고 있습니다. 토론 중인 내용을 일방적으로 편집할 경우 관련 규정에 따라 제재될 수 있습니다.주의. 사건·사고 관련 내용을 설명합니다. 이 문서는 실제로 일어난 사건·사고의 자세한 내용과 설명을 포함하고 있습니다. 사건 관련 뉴스 영상 [ 속보 ] [ 현장 브리핑 ]사건 요약발생일시2021년 6월 9일 16시 23분발생 위치광주광역시 동구 남문로 717 학산빌딩[지번](학동·증심사입구역 시내버스 정류장) #관할 관서 광주소방안전본부(동부소방서) 광주광역시경찰청(동부경찰서)유형붕괴원인조사 중인명피해사망9명부상8명(중상: 8명)구조8명재산 피해집계 중교통정보센터 CCTV[2][카카오맵][네이버지도]1. 개요2. 경과2.1. 6월 9일2.2. 6월 10일3. 대응3.1. 사고 관계자3.2. 사고 수습 기관3.3. 정부3.3.1. 대통령3.3.2. 국무총리3.3.3. 행정부처3.4. 지방자치단체4. 분석4.1. 피해4.2. 원인5. 기타6. 피해자들의 사연7. 미디어7.1. 방송7.2. 관련 기사8. 유사 사고철거 공사 이전의 학산빌딩 건물사고 이후 완전히 붕괴된 학산빌딩 현장 CCTV2021년 6월 9일 16시 23분경 광주광역시 동구 학동에서 학동4구역 재개발을 위해 HDC현대산업개발의 하도급업체인 한솔기업이 철거하던 학산빌딩이 붕괴된 사고이다.16시 23분: 재개발을 위해 철거 중인 건물이 무너졌다. 이 순간 정류장에 정차한 운림54번 버스가 매몰되었다. CCTV 영상에서는 각도에 따라 버스 2대가 동시에 매몰되는 것처럼 보이기도 하지만, 주행 중이던 버스 1대[5]는 간발의 차로 정지해 있던 운림54번 버스를 추월해 앞으로 빠져나갔다. 아래 사진의 인도 쪽에서 대피하는 작업자를 볼 수 있듯이, 작업자들은 붕괴 조짐을 알고 미리 대피하여 있던 상황이였다.붕괴 순간의 학산빌딩 건물16시 31분: 소방 대응 1단계가 발령되었다.16시 40분: 소방 대응 2단계가 발령되었다. 광주소방본부 특수구조단과 광주 시내 5개 소방서가 투입되었다.19시 07분: 버스 앞쪽에 있던 8명이 먼저 구조되어 병원으로 이송된 후, 매몰자 2명이 추가 구조됐으나 숨졌다. 남은 매몰자는 2명으로 판단 중이다. 피해자는 총 11명(사망 3명, 부상 8명)[6]이 되었다.매몰자 2명을 구조하는 소방관들[7]19시 40분경: 매몰자 1명이 수습되었다. 남은 매몰자는 1명으로 판단 중이다. 피해자는 총 12명(사망 4명, 부상 8명)이 되었다.20시 00분경: 매몰자 1명이 수습되었다. 기존에 판단한 매몰자 1명 이외에 다른 1명이 추가로 수습된 것이다. 피해자는 총 13명(사망 5명, 부상 8명)이 되었다.20시 12분경: 매몰자 1명이 수습되었다. 기존에 판단한 매몰자 1명 이외에 다른 1명이 추가로 수습된 것이다. # 피해자는 총 14명(사망 6명, 부상 8명)이 되었다. 남은 매몰자 1명은 생사불명으로 압착된 버스에 갇혀있는 사람이라고 한다.20시 15분경: 남은 매몰자 1명을 포함한 총 4명이 수습되었다. 피해자는 총 17명(사망 9명, 부상 8명)이다. 추가 피해자는 없을 것으로 판단하지만, 구조 작업은 계속 진행 중이다.20시 30분경: 매몰된 시내버스를 잔해 밖으로 끌어냈다.매몰되었던 버스[8]11시경: 사고현장이 완전히 수습되고 구조작업도 종료됐다. 다만 13시경 예정된 현장감식을 위해 도로 일부가 통제되고 있다.한솔기업 (하도급업체)[9]HDC현대산업개발 (도급 원청)하도급을 한 원청 HDC현대산업개발의 권순호 대표이사·사장은 사고 현장을 찾아서 광주 시민들에게 사과했다. #1 #2 이후 '재하도급 의혹'이 제기되자 그는 '하도급업체인 한솔기업 이외에 재하도급을 준 적이 없다'며 관련 의혹을 부인했다. 기사 영상사고 다음날인 6월 10일 오전, 정몽규 HDC그룹 회장이 광주광역시청 브리핑룸을 찾아서 “이번 사고에 대해 진심으로 사죄드리며 무거운 책임을 통감하고 있다”라면서 '피해자·유가족의 피해 회복 및 조속한 사고 수습, 그리고 재발 방지에 최선을 다하겠다'고 밝혔다. 영상정 회장의 발언이나 여타 관련 언급들(이용섭 시장)을 종합하면, HDC그룹은 원인 규명과 관계 없이 피해자와 유가족에게 피해 보전(補塡)하는 쪽으로 입장을 정리한 것으로 보인다. 소방청사고 발생 직후 '대응 1단계'를 발령했고, 곧 '대응 2단계'로 격상했다. 사고현장에는 장비 63대와 대원 480여명이 투입됐다.신열우 소방청장이 사고 직후 현장으로 급파됐다. #소방 및 경찰당국은 사고 현장 옆에 위치한 삼성전자 디지털프라자 앞마당에 대응본부를 설치했다. 사건 현장에 굴삭기 4~5대가 투입되어 잔해물을 제거 중이다.경찰청사고 당일 (6월 9일)광주광역시경찰청은 강력범죄수사대를 중심으로 하여 전담수사팀을 구성한다. 안전수칙 준수 여부와 업무상 과실 여부를 수사한다고 밝혔다. #1 #2 철거 현장 관계자와 목격자 진술을 청취하며 사고 경위를 파악하고 있으며, 사고 현장에 순찰차와 인원을 배치하여 2차 사고 예방 및 교통 통제 중이라고 한다.6월 10일국가수사본부는 오전 중 '합동수사팀'이 '합동수사본부'로 격상되고 '광주광역시경찰청 수사부장'이 본부장을 맡게 되었다고 밝혔다. #13시경에 국립과학수사연구원과 현장 감식을 합동 진행할 계획이다. 문재인 대통령은 사고 다음날에 \"희생자들의 명복을 빌고 피해자와 가족, 그리고 더 나아가 광주시민들에게 깊은 위로의 마음을 전한다\"라면서 관계 기관에 사망자 장례 절차와 부상자 치료 지원과 철저한 사고 원인 조사와 책임 소재를 규명할 것을 지시했다. #김부겸 국무총리는 사고 당일 \"가용한 모든 장비와 인력을 동원해 신속하게 매몰자를 구조하고 인명피해 최소화를 위해 모든 조치를 강구하라\"는 지시를 하면서, 노형욱 국토교통부 장관과 전해철 행정안전부 장관에게 \"추가 피해가 발생하지 않도록 안전 조치를 취하라\"라는 별도 지시를 했다. #행정안전부전해철 행정안전부 장관은 사고 발생 직후 정부서울청사 서울상황센터에서 긴급대책회의를 주재하고, \"광주광역시와 동구 등 지방자치단체와 소방·경찰 등 모든 가용자원을 총동원해 매몰자 등 인명구조에 총력을 다하고 구조과정에서 소방대원의 안전 확보에도 만전을 기하라\"라고 지시했다. #전해철 장관은 같은 날 23시에 사고 현장을 방문했다. #1 #2 여기서 전 장관은 희생자와 유가족에게 위로의 말을 전하며 \"동원 가능한 장비와 인력을 총동원해 혹시라도 구조가 필요한 사람이 있는지 인명구조를 철저히 해달라\"라면서 \"사상자 신원을 신속히 파악해 가족들에게 세부 상황을 알리고, 각 가족마다 전담 공무원을 지정해 편의제공에 최선을 다해달라\"라는 주문을 했다.국토교통부기술안전정책관과 익산지방국토관리청장 등을 급파해 현장 수습을 지원하고 사고수습본부를 구성했다. #노형욱 장관은 중앙사고수습본부로부터 사고수습 현황과 조치계획을 보고받고 \"피해자와 가족들을 위해 지원할 방안을 다각적으로 검토하고 조치에 최선을 다해 달라\"고 당부했으며, \"관계부처와 함께 취약한 철거현장을 신속히 점검하고, 근본적인 재발방지 대책을 마련해 국민 불안을 해소할 수 있도록 최선을 다하겠다\"고 말했다. #광주광역시이용섭 광주광역시장은 사고 현장을 지키며 상황 수습에 나섰다. 기사이 시장은 사고 다음날 오전 광주광역시청 브리핑실에서 희생자들에게 애도를 표하고 사고 수습과 철저한 원인 조사, 재발 방지를 약속했다. 영상동구임택 동구청장은 사고 다음날 새벽에 사고 현장에서 경과와 후속 대책을 설명했다. 기사임 구청장도 이용섭 시장에 뒤이어 입장을 발표하며 사고 수습에 최선을 다하겠다고 밝혔다. 영상붕괴 건물 철거를 맡은 시공사와 감리자를 고발하기로 했다. 기사인적 피해 (총 17명) # 사망자 9명(10대 남성 1명, 20대 여성 1명, 40대 여성 1명, 60대 남성 1명, 60대 여성 4명, 70대 여성 1명)부상자 8명[10]깔린 버스 안에서 전화통화를 한 승객은 구조되어 치료를 받고 있다. #물적 피해대창운수 소속 시내버스 1대 전손. #1 #2사고차량은 CNG(압축천연가스) 차량이라 가스 폭발의 위험성이 있었으나, 기적적으로 폭발하지 않아 생존자가 나올 수 있었다. 소방대는 가스 누출을 탐지하고 가스 안전조치를 취한 뒤 구조작업을 실시하였다. #버스 정류장 2개 및 가로수, 도로 시설물 파손인근 건물 및 주민이 입은 손해는 미상2021년 6월 10일 11시 기준 공식적으로 확인된 사고 원인은 없다. 다만 재개발을 위한 철거 작업이 진행 중이었으므로, 이 과정에서 안전 관리에 허점이 있었을 것으로 추정된다.이날 방송한 MBC 뉴스데스크의 보도에 따르면,# 가림막 이외에는 안전장치가 보이지 않았고 이 때문에 주민들이 불안하게 여겼다고 한다.[11] 또한 사고 당시 차량 및 보행자 통제가 제대로 이뤄지지도 않았다고 한다. 한편 공사 관계자들은 이상 조짐을 느끼고 모두 대피했으면서도 주변 사람들에게 위험을 알리는 등의 조치를 취하지도 않았다고 한다.같은 날 방송한 SBS 8 뉴스 보도에 따르면,# 경찰과 동구청, 소방당국은 '건물 무게를 지탱하는 부분을 먼저 철거했을 가능성', '철제 기둥을 세워서 무게를 분산하지 않았을 가능성', '콘크리트 잔해 등 하중이 될 만한 걸 방치했을 가능성' 등을 염두에 두고 수사 중이라고 한다.건물 뒤편에 토사물을 무리하게 쌓아 붕괴되었다는 주장이 제기되었다. 철거하면서 생긴 토사물은 1층으로 무조건 반출해야 하는데 반출하지 않아 건물에 하중에 무리가 생겨 붕괴된 것이라는 주장이다. 그리고 굴착기도 10톤 이상짜리를 옥상에 올렸다는 의혹도 있다. #[12]작업 방법부터가 문제였다는 주장이 있다. 보도에 따르면 3개 층의 벽체를 나중에 한번에 쓰러뜨리려고 남겨 놓는 방식을 썼다고 하는데, 해당 철거 방법은 주변에 사람이 있을 수가 없는 대규모 철거 현장의 내부에서나 비용 절감을 위해 쓰는 방법이다. 원칙으로 지상건축물 해체는 옥탑, 슬래브, 작은보, 큰보, 비내력벽, 내력벽, 기둥 순으로 해체하는게 원칙이고 절대로 옆면 기둥을 제거하면 안 된다. 게다가 해당 방법으로 건물을 철거하게 되면, 사고 없이 정상적으로 진행해도 사방으로 파편이 튀기 마련이다. 당연하게도 건물 3개 층이 한꺼번에 무너질 때 튀어오르는 콘크리트 파편에 사람이 맞으면 최소 중상이다. 바로 옆에 버스가 지나다니는 대로변에서 할 만한 방식이 아니라는 것. 저런 대로변에선 무조건 1개 층씩 차례차례 철거해야 하며, 특히 밖으로 파편이 튈 가능성이 있는 벽체 철거시엔 도로까지 잠시 통제한 뒤 철거하는 것이 일반적이다. 당장 버스가 매몰된 것만 봐도 도로 통제니 뭐니 아무것도 이루어지지 않았음을 알 수 있다.[13] 그리고 안전하게 하려면 10톤 미만 굴착기을 옥상에 올려서 1층씩 내려와 어느정도 내려오면 지상에서 굴착기로 철거해야 한다.철거하는 모습불법전도 하는 모습[14]2~3층부터 철거하는 모습불행 중 다행으로 사고가 난 지역은 상급종합병원인 조선대학교병원의 바로 앞이자 전남대학교병원에서 차로 3분 거리다. 덕분에 구조단과 의료진과의 협업이 잘 이루어졌다. 하지만 물리적인 충격이 심하게 가해져서 많은 사망자가 발생하였고 생존자 전원이 중상을 입었다.사고 발생 초기에 공사장 근로자와 시내버스 1대, 승용차 2대가 매몰되었다고 보도되었으나 이후 시내버스 1대만 매몰된 것으로 정정되었다.이로 인해 학동·증심사입구역 일대를 지나는 노선버스들은 남광주시장부터 삼익세라믹아파트까지 천변좌로, 천변우로로 사고 수습 시점까지 한시적으로 우회 운행하고 있다.붕괴된 건물 쪽 방면 3개 차로가 현재 차단 중이다. 중앙선을 넘어 유도로를 만들어 통행할 수 있도록 해 놓았다.사망자 9명은 모두 버스 뒷좌석 탑승객이었다. 앞좌석은 피해가 덜해, 앞좌석 탑승객 중 1명은 버스가 잔해에 깔린 직후 자신의 휴대전화를 사용해 스스로 119에 구조요청을 한 것으로 알려졌다. 소방당국의 발표에 따르면 앞좌석 쪽은 잔해에 함께 깔린 가로수가 완충장치 역할을 해 충격을 직접 받은 뒷좌석 쪽에 비해 피해가 적었다고 한다. #깔린 버스에 탔던 60대 여성[15]은 아들의 생일을 위해 시장을 갔다 오다 두 정거장 남겨두고 사망하는 안타까운 소식이 전해졌다. #사망자 중에는 10대 사망자인 광주의 한 고등학교 2학년 학생이 포함되어 있다. 늦둥이인 외아들이 집에 들어올 시간이 되었는데 집에 들어오지 않고, 뉴스에선 철거 중인 건물이 도로의 시내버스를 덮쳤다는 소식이 나오고 있어 희생자의 어머니가 바로 사고현장으로 달려가 아들을 찾았다. 고등학생 희생자는 사망자 9명 중 가장 늦게 시신이 수습되어 안타까움을 자아냈다. 고등학생이 집까지 오는데 남겨둔 버스 정류장의 수는 네 정거장이었다. 비대면 수업기간이었으나 동아리 활동 때문에 외출했다가 귀가하는 중 변을 당했다. #1 #2 #3 #4피해자들 중 부녀관계도 있다. 함께 버스를 탄 상태에서 화를 입었는데 앞좌석에 앉은 아버지는 겨우 생존하고 20대 후반인 막내딸은 사망하는 참혹한 변을 당했다. 공무원 시험을 준비하던 고인은 엄마를 만나러 가다가 변을 당했다. #사망자 중 70대 여성은 봉사활동을 끝내고 집으로 돌아가던 중 변을 당했다. #버스 운전기사는 구조되어 치료를 받고 있다.KBS는 유튜브 채널을 통해서 1시간 가량 실시간 속보 방송을 진행했고 지역 주민으로부터 제보 영상[16]을 입수하여 최초 보도하기도 했으나, 19시가 되기 10여 분 전부터 방송 상태가 몹시 불안정해서 제대로 보도가 이뤄지지 못했다.연합뉴스TV와 YTN, JTBC는 중간중간 현장 상황을 보도했으며, 전술한 보도전문채널 두 개는 소방당국의 현장 브리핑을 실시간으로 보도했다.SBS는 사고에 휩쓸리는 걸 간신히 면한 어느 차량의 블랙박스 영상을 입수해서 유튜브를 통해 가장 일찍 공개했다.\"쾅! 굉음 3 ~ 4초만에 버스 덮쳐\"..목격자들 경악'고교생 사망자도'..광주서 건물 무너져 9명 사망·8명 중상\"뚝뚝\" 붕괴 징후에도 통제 안한 관계자들..무너지기 직전 피했다큰아들 생일 미역국 끓여놓고 장사 나가, 사고 직전 통화했는데..막내딸은 뒷좌석, 아버지는 앞좌석.. 매몰된 버스, 부녀 생사는 갈렸다잠원동 철거건물 붕괴사고\n"
     ]
    }
   ],
   "source": [
    "# 페이지 내 텍스트 구조확인\n",
    "\n",
    "req = scraper.get(page_urls[0])\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name=\"article\")\n",
    "title = contents_table.find_all('h1')[0]\n",
    "category = contents_table.find_all('ul')[0]\n",
    "content_paragraphs = contents_table.find_all(name='div', attrs={'class':'wiki-paragraph'})\n",
    "content_corpus_list = []\n",
    "\n",
    "for paragraphs in content_paragraphs:\n",
    "    content_corpus_list.append(paragraphs.text)\n",
    "content_corpus = \"\".join(content_corpus_list)\n",
    "\n",
    "print(title.text)\n",
    "print('\\n')\n",
    "print(category.text)\n",
    "print('\\n')\n",
    "print(content_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff99634-9e5c-4d85-bace-a2429eeaafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 웹 크롤링을 불러왔으면...\n",
    "# 데이터 프레임을 만들 후 넣어주는 작업을 해야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd3c8dd-20ee-4591-ab81-e8252aa35657",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-05cca7636b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'category'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'content_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "\n",
    "columns = ['title', 'category', 'content_text']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "# 각 페이지별 '제목', '카테고리', '본문' 정보를 데이터 프레임으로 만듬\n",
    "\n",
    "for page_url in page_urls:\n",
    "    \n",
    "    # Site html 구조에 기반하여 크롤링을 수행\n",
    "    req = scraper.get(page_urls[0])\n",
    "    html = req.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf58cd-b4ba-4ec8-aa0c-ea9813d7dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c74f94-9947-4d60-8ed5-b5db84488671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "columns = ['title', 'category', 'content_text']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 각 페이지별 '제목', '카테고리', '본문' 정보를 데이터 프레임으로 만듭니다.\n",
    "for page_url in page_urls:\n",
    "\n",
    "    # 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "    req = scraper.get(page_url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    contents_table = soup.find(name=\"article\")\n",
    "    title = contents_table.find_all('h1')[0]\n",
    "    \n",
    "    # 카테고리 정보가 없는 경우를 확인합니다.\n",
    "    if len(contents_table.find_all('ul')) > 0:\n",
    "        category = contents_table.find_all('ul')[0]\n",
    "    else:\n",
    "        category = None\n",
    "        \n",
    "    content_paragraphs = contents_table.find_all(name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "    content_corpus_list = []\n",
    "    \n",
    "    # 페이지 내 제목 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if title is not None:\n",
    "        row_title = title.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_title = \"\"\n",
    "    \n",
    "    # 페이지 내 본문 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if content_paragraphs is not None:\n",
    "        for paragraphs in content_paragraphs:\n",
    "            if paragraphs is not None:\n",
    "                content_corpus_list.append(paragraphs.text.replace(\"\\n\", \" \"))\n",
    "            else:\n",
    "                content_corpus_list.append(\"\")\n",
    "    else:\n",
    "        content_corpus_list.append(\"\")\n",
    "        \n",
    "    # 페이지 내 카테고리정보에서 “분류”라는 단어와 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if category is not None:\n",
    "        row_category = category.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_category = \"\"\n",
    "    \n",
    "    # 모든 정보를 하나의 데이터 프레임에 저장합니다.\n",
    "    row = [row_title, row_category, \"\".join(content_corpus_list)]\n",
    "    series = pd.Series(row, index=df.columns)\n",
    "    df = df.append(series, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
